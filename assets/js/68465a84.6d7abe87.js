"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[249],{4133:(e,a,i)=>{i.r(a),i.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>n,metadata:()=>r,toc:()=>c});var t=i(4848),s=i(8453);const n={title:"Case Study",description:"Willow Case Study"},o="Willow Case Study",r={type:"mdx",permalink:"/case-study",source:"@site/src/pages/case-study.md",title:"Case Study",description:"Willow Case Study",frontMatter:{title:"Case Study",description:"Willow Case Study"},unlisted:!1},l={},c=[{value:"0 - Introduction",id:"0---introduction",level:2},{value:"1 - Background",id:"1---background",level:2},{value:"1.1 - General Problem Domain",id:"11---general-problem-domain",level:3},{value:"1.2 - Caching",id:"12---caching",level:3},{value:"1.3 - Change Data Capture",id:"13---change-data-capture",level:3},{value:"1.3.1 Timestamp",id:"131-timestamp",level:4},{value:"1.3.2 Trigger",id:"132-trigger",level:4},{value:"1.3.3 Log-based",id:"133-log-based",level:4},{value:"2 - Existing Solutions",id:"2---existing-solutions",level:2},{value:"2.1 - Enterprise Solutions",id:"21---enterprise-solutions",level:3},{value:"2.2 - DIY Solutions",id:"22---diy-solutions",level:3},{value:"3 - Introducing Willow",id:"3---introducing-willow",level:2},{value:"3.1 Demonstration",id:"31-demonstration",level:3},{value:"3.2 - Using Willow",id:"32---using-willow",level:3},{value:"4 - Implementation",id:"4---implementation",level:2},{value:"4.1 - Debezium",id:"41---debezium",level:3},{value:"4.2 - Apache Kafka",id:"42---apache-kafka",level:3},{value:"4.3 - Willow Adapter",id:"43---willow-adapter",level:3},{value:"4.4 - PostgreSQL",id:"44---postgresql",level:3},{value:"5 - Architecture",id:"5---architecture",level:2},{value:"6 - Challenges",id:"6---challenges",level:2},{value:"6.1 - Debezium Configuration",id:"61---debezium-configuration",level:3},{value:"6.1.1 - Multiple Pipelines Sharing a Replication Slot",id:"611---multiple-pipelines-sharing-a-replication-slot",level:4},{value:"6.1.2 - Working with Minimum Privileged User",id:"612---working-with-minimum-privileged-user",level:4},{value:"6.2 - Event Transformation",id:"62---event-transformation",level:3},{value:"6.2.1 - Tombstone Events",id:"621---tombstone-events",level:4},{value:"6.2.2 - No Primary Key",id:"622---no-primary-key",level:4},{value:"6.2.3 - Composite Primary Key",id:"623---composite-primary-key",level:4},{value:"7 - Conclusion and Future Work",id:"7---conclusion-and-future-work",level:2},{value:"8 - References",id:"8---references",level:3}];function d(e){const a={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.h1,{id:"willow-case-study",children:"Willow Case Study"}),"\n",(0,t.jsx)(a.h2,{id:"0---introduction",children:"0 - Introduction"}),"\n",(0,t.jsx)(a.p,{children:"Willow is an open-source, self-hosted framework for building log-based change data capture pipelines that update caches in near real-time based on changes in a database."}),"\n",(0,t.jsx)(a.p,{children:"Willow abstracts away the complexity of setting up and configuring open source tools like Debezium and Apache Kafka. Utilizing log-based change data capture, Willow non-invasively monitors changes in a user's PostgreSQL database and reflects those row-level changes in a user's Redis cache. Without requiring in-depth technical knowledge and expertise, data pipelines can be created or deleted using Willow's web user interface, simplifying setup and teardown."}),"\n",(0,t.jsx)(a.p,{children:"This case study provides background on caching and its tradeoffs, explains change data capture, and discusses why developers would use it to keep a cache consistent. Next, Willow's implementation and architecture are explored, and challenges encountered when creating Willow and how these were addressed is explained. Finally, Willow's roadmap for the future is outlined."}),"\n",(0,t.jsx)(a.h2,{id:"1---background",children:"1 - Background"}),"\n",(0,t.jsx)(a.h3,{id:"11---general-problem-domain",children:"1.1 - General Problem Domain"}),"\n",(0,t.jsx)(a.p,{children:"A web application's response time significantly influences user experience and company profitability. An Amazon study found that every 100ms of latency dropped sales by 1%. Google's research found that a 100ms improvement in mobile site speeds increased how many visitors made purchases by up to 10%. Web applications are growing increasingly complex and distributed, forcing developers to find ways to improve application speed and response time."}),"\n",(0,t.jsx)(a.p,{children:"One source of slowdown is an application's database. Database queries can become a limiting factor for an application\u2019s response time for various reasons, including when multiple applications are simultaneously accessing the database or when unoptimized queries block other queries from being executed. A common way to increase read response times is to utilize a cache."}),"\n",(0,t.jsx)(a.h3,{id:"12---caching",children:"1.2 - Caching"}),"\n",(0,t.jsx)(a.p,{children:"Caching is a strategy to improve web application performance by taking demand off of source databases and providing speedy access to data. With caching, frequently accessed data is stored in a temporary location. This is usually an in-memory form of storage, which means data can be accessed faster than persistent, disk-based storage. By utilizing caching, an application checks the cache for data before requesting it from the database. This can reduce read demands on the source, taking pressure off of the database and increasing the speed of the application overall."}),"\n",(0,t.jsx)(a.p,{children:"However, there are tradeoffs when using a cache. One major challenge is keeping data in the cache up-to-date and consistent with the source of truth. This issue is known as cache consistency."}),"\n",(0,t.jsx)(a.p,{children:"Caching is a viable strategy when there is only one application making changes to the source database. When the architecture is simple, caches can improve performance while avoiding cache inconsistency. This is the case with many types of caching strategies: read-through, write-through, and even cache-aside. Each of these strategies positions the cache either beside the application or between the application and the database. With read-through and cache-aside approaches, the cache is checked for the requested data before the application checks the database. If the data is not found in the cache, this is called a cache miss, and requires the application or cache to check the database for the missing data. Once the queried data is retrieved, it is then stored in the cache for future requests. A write-through cache mirrors what\u2019s in the database, as any writes to the database are first sent to the cache, written there, and then forwarded to the database."}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/1.2-Cache-Aside.svg",alt:"A cache aside strategy"}),(0,t.jsx)("figcaption",{children:"Cache aside"})]}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/1.2-Cache-Write-through.svg",alt:"A write-through cache strategy"}),(0,t.jsx)("figcaption",{children:"Write-through"})]}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/1.2-Cache-Read-Through.svg",alt:"A read-through cache strategy"}),(0,t.jsx)("figcaption",{children:"Read-through"})]}),"\n",(0,t.jsxs)(a.p,{children:["Cache inconsistency becomes a problem when another application or component makes updates to the source database. Because the cache is unaware of these changes, it is possible for the cache to become inconsistent with the source and serve data that is no longer in sync with the source database. This out-of-sync data is referred to as ",(0,t.jsx)(a.strong,{children:"stale data"}),"."]}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/1.2-Complex-Architecture.gif",alt:"Cache inconsistency occurs in complex architectures"}),(0,t.jsx)("figcaption",{children:"Cache inconsistency occurs in complex architectures"})]}),"\n",(0,t.jsx)(a.p,{children:"There are cache invalidation strategies, such as Time-To-Live (TTL) and polling, that attempt to mitigate this problem."}),"\n",(0,t.jsx)(a.p,{children:"TTL is a period of time that a value should exist in a cache before being discarded. This approach aims to minimize the presence of stale data. After the data expires and the application\u2019s query to the cache results in a cache miss, the application queries the source database and repopulates the cache. This process incurs additional resource costs and increases demand on the source, especially for expired data that is not stale. This demand is exacerbated when the expiration time is made shorter, and checks on the database for updated data increase. Likewise, longer TTL periods increase the likelihood of stale data, which can be unacceptable for applications requiring timely data accuracy. While TTL can be fairly straightforward to implement, choosing the TTL value that reduces stale data and minimizes database queries is difficult."}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/1.2-TTL Too Short.gif",alt:"Short TTL can result in unnecessary network requests"}),(0,t.jsx)("figcaption",{children:"Short TTL can result in unnecessary network requests"})]}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/1.2-TTL Too Long.gif",alt:"Long TTL can result in retrieving stale data"}),(0,t.jsx)("figcaption",{children:"Long TTL can result in retrieving stale data"})]}),"\n",(0,t.jsx)(a.p,{children:"An alternative cache invalidation strategy, polling is when the cache or application periodically checks if the cache\u2019s data is inconsistent with the source database, updating any inconsistent data. Polling typically happens on fixed intervals, such as every 10 seconds, to ensure data consistency. While polling can decrease the time that data remains stale, it also puts extra demand on the source database by running frequent queries."}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/1.2-polling.svg",alt:"Polling places additional strain on the database"}),(0,t.jsx)("figcaption",{children:"Polling places additional strain on the database"})]}),"\n",(0,t.jsx)(a.p,{children:"These cache invalidation strategies - TTL and polling - consist of seeking out and pulling changes from the database. For systems that don\u2019t require constant data consistency and can endure brief moments of stale data, these strategies may be appropriate. However, TTL and polling are not appropriate for systems that require fresh data in near real-time. In the next section, we examine change data capture, a strategy that is not only more appropriate for these types of systems, but can put less demand on the source database."}),"\n",(0,t.jsx)(a.h3,{id:"13---change-data-capture",children:"1.3 - Change Data Capture"}),"\n",(0,t.jsx)(a.p,{children:"Change data capture has many use cases, including keeping caches in sync with databases."}),"\n",(0,t.jsxs)(a.p,{children:["Change data capture (CDC) refers to the process of identifying and capturing changes in a ",(0,t.jsx)(a.strong,{children:"source"})," - a system that provides data - and then delivering those changes in near real-time to a ",(0,t.jsx)(a.strong,{children:"sink"})," - a system that receives data. Near real-time (or \u201csoft real-time\u201d) refers to the processing of data where systems can tolerate slight delays from a few seconds to a few minutes. This is where most networked communication lies and is opposed to \u201chard real-time\u201d systems, where data processing is benchmarked against 250 milliseconds (the average human response time) of delay."]}),"\n",(0,t.jsx)(a.p,{children:"CDC can be implemented through a series of different methods, each having their own benefits and tradeoffs."}),"\n",(0,t.jsx)(a.h4,{id:"131-timestamp",children:"1.3.1 Timestamp"}),"\n",(0,t.jsxs)(a.p,{children:["Timestamp-based CDC involves adding metadata columns (e.g., ",(0,t.jsx)(a.code,{children:"created_at"}),", ",(0,t.jsx)(a.code,{children:"updated_at"}),") to each database table. One limitation is the inability to perform permanent, hard deletes of rows. Metadata of all changes, including deletes, must persist in order to detect changes. Additionally, to keep the target system in sync, the database needs to be regularly queried for changes, putting additional overhead on the source system."]}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/1.3-timestamp.svg",alt:"A database table demonstrating the timestamp-based CDC method. The updated at column is used to remember when a row was last updated."})}),"\n",(0,t.jsx)(a.h4,{id:"132-trigger",children:"1.3.2 Trigger"}),"\n",(0,t.jsxs)(a.p,{children:["Trigger-based CDC relies on the database\u2019s built-in functionality to invoke a custom function, or trigger, whenever a change is made to a table. Changes are usually stored in a different table within the same database called a shadow table or event table. A shadow table is essentially a time ordered changelog of all operations performed in the database, providing visibility for ",(0,t.jsx)(a.code,{children:"INSERT"}),", ",(0,t.jsx)(a.code,{children:"UPDATE"}),", and ",(0,t.jsx)(a.code,{children:"DELETE"})," changes."]}),"\n",(0,t.jsx)(a.p,{children:"While most database systems support triggers, this method has drawbacks. One drawback of trigger based CDC is every trigger requires an additional write operation to an event table. These additional writes impact database performance, especially at scale for write-heavy applications. Another limitation is the event table must be queried to propagate changes to any downstream processes."}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/1.3-trigger.svg",alt:"Two database tables demonstrating the trigger-based CDC method. Individual changes that occur in an Employees table are recorded in an Event table."})}),"\n",(0,t.jsx)(a.h4,{id:"133-log-based",children:"1.3.3 Log-based"}),"\n",(0,t.jsx)(a.p,{children:"Log-based CDC involves leveraging the database transaction log \u2014 a file that keeps a record of all changes made to the database \u2014 for capturing change events and delivering those to downstream processes. In log-based CDC, the database transaction log is asynchronously parsed to determine changes instead of formally querying the database. Hence, the log-based method is the least invasive out of the three methods, requiring the least additional computational overhead on the source database."}),"\n",(0,t.jsx)(a.p,{children:"Although it offers superior performance and reduced latency, the log-based method comes with its own set of tradeoffs. Database transaction log formats are not standardized, so logs between database management systems can vary and vendors can change log formats in future releases. Custom code connectors are also needed in order to read from a transaction log. Additionally, these logs usually only store changes for a particular retention period."}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/1.3-log.svg",alt:"Image showing the general structure of the log-based CDC method. A CDC connector reads the database's transaction log to determine changes."})}),"\n",(0,t.jsx)(a.p,{children:"As mentioned, CDC can be used to build a low-latency data pipeline that propagates changes from a database to a cache. Out of the three, log-based CDC takes a non-invasive approach. Because it minimizes impact to database performance, we looked at log-based approaches when considering existing solutions."}),"\n",(0,t.jsx)(a.h2,{id:"2---existing-solutions",children:"2 - Existing Solutions"}),"\n",(0,t.jsx)(a.p,{children:"Log-based CDC is popular among applications requiring up-to-date data in near real-time. Developers have several options for building a log-based CDC data pipeline to replicate data from a source database to a sink cache. Multiple enterprise solutions exist, as well as various open-source projects that can be combined for a custom DIY solution. When deciding which solution to pursue, developers should consider  scalability, ease of use and connector types available."}),"\n",(0,t.jsx)(a.h3,{id:"21---enterprise-solutions",children:"2.1 - Enterprise Solutions"}),"\n",(0,t.jsx)(a.p,{children:"Multiple enterprise solutions using CDC to replicate data from a source database to a sink cache are available. Prominent solutions include Redis Data Integration and Confluent. Both take care of managing the CDC pipeline and have a number of available source and sink connectors - applications capable of either extracting changes from a data source or replicating changes to a data destination. Redis Data Integration and Confluent are built on top of open-source tools (Debezium and Apache Kafka) and provide additional benefits, like a wide selection of source and sink connectors, architecture management, and built-in scalability."}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/2.1-Confluent-Redis.svg",alt:"Confluent and Redis logos."})}),"\n",(0,t.jsx)(a.p,{children:"Enterprise solutions are a good fit for well-funded development teams that want a third-party to manage their architecture and hosting logistics. However, enterprise solutions come with tradeoffs, including vendor lock-in, recurring costs, and reduced infrastructure control. In particular, developers do not have control over how or when infrastructure is upgraded or maintained, which can result in service downtime."}),"\n",(0,t.jsx)(a.h3,{id:"22---diy-solutions",children:"2.2 - DIY Solutions"}),"\n",(0,t.jsx)(a.p,{children:"An alternative to enterprise solutions, DIY solutions can be built by leveraging open-source tools, like Debezium and Apache Kafka. These tools are open-source, provide a high level of data customization, and offer a wide number of community-maintained source and sink connectors. Customizations include but aren\u2019t limited to filtering data, transforming data, aggregating data, and horizontally scaling CDC pipelines."}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/2.2-Debezium-Kafka.svg",alt:"Debezium and Kafka logos."})}),"\n",(0,t.jsx)(a.p,{children:"Building a DIY solution using open-source tools is a good fit for development teams that prefer to manage their architecture, have a high level of control and customization in their CDC pipeline, and avoid recurring costs from enterprise providers. However, DIY solutions require significant and complex configuration of open-source tools, and developers must provision and manage appropriate infrastructure for the CDC pipeline. The time to learn, implement, and configure these technologies can slow down teams looking to quickly deploy a CDC pipeline."}),"\n",(0,t.jsx)(a.h2,{id:"3---introducing-willow",children:"3 - Introducing Willow"}),"\n",(0,t.jsx)(a.p,{children:"Given the tradeoffs that accompany enterprise CDC solutions and the complexity of a DIY solution, our team identified a gap in the solution space. Willow was developed as an open source, user-friendly framework designed to maintain cache consistency by creating a near real-time CDC pipeline that monitors changes in a user's PostgreSQL database and reflects row-level changes in a user's Redis cache."}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/3-comparison_table.svg",alt:"Table comparing Willow against enterprise solutions and DIY solutions. Comparison criteria are no vendor lock in, easy to configure, infrastructure control, large number of connectors, and cost."})}),"\n",(0,t.jsx)(a.p,{children:"Willow\u2019s user-friendly UI abstracts away configuration complexities we encountered in the DIY solutions by guiding users through a set of forms to set up a pipeline. Once a pipeline is created, users can set up additional pipelines, view a list of existing pipelines and their configuration details, and delete a pipeline."}),"\n",(0,t.jsx)(a.p,{children:"Willow can be deployed on users\u2019 server of choice, allowing users to retain infrastructure control and avoiding vendor lock-in. While Willow only has a single source connector for PostgreSQL and a single sink connector for Redis, the simplicity of setting up and configuring these connectors into a CDC pipeline reduces overall deployment time and cost."}),"\n",(0,t.jsx)(a.p,{children:"Willow pipelines connect a PostgreSQL database and a Redis cache, storing database rows in the Redis cache as JSON objects Updates made to the database are passed on to the cache in near real-time."}),"\n",(0,t.jsx)(a.h3,{id:"31-demonstration",children:"3.1 Demonstration"}),"\n",(0,t.jsxs)(a.p,{children:["The best way to understand what Willow does is by seeing it in action. In the video below, a PostgreSQL terminal logged into the ",(0,t.jsx)(a.code,{children:"willow"})," database is on the left and RedisInsight - a Redis GUI for visualizing a cache's contents - is on the right. A Willow CDC pipeline connects the source PostgreSQL instance on the left to the sink Redis cache on the right."]}),"\n",(0,t.jsxs)(a.p,{children:["Initially, the PostgreSQL ",(0,t.jsx)(a.code,{children:"store"})," table and the Redis cache are empty. Once a row is inserted into ",(0,t.jsx)(a.code,{children:"store"}),", Willow replicates the row in the cache. After refreshing RedisInsight, we can see that the row inserted into our PostgreSQL table has been replicated in our Redis cache."]}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/3.1-demo.gif",alt:"A demonstration of Willow. On the left side of the screen is a PostgreSQL terminal. On the right side of the screen is a Redis cache shown in RedisInsight. An INSERT command is performed in the PostgreSQL terminal, and the inserted data automatically appears in the Redis cache."})}),"\n",(0,t.jsx)(a.h3,{id:"32---using-willow",children:"3.2 - Using Willow"}),"\n",(0,t.jsxs)(a.ol,{children:["\n",(0,t.jsx)(a.li,{children:'Initially, users are greeted with a "Welcome to Willow" page, offering an invitation to create a CDC pipeline with a click of a button.'}),"\n"]}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/3.1-1_home.png",alt:"Willow's home page."})}),"\n",(0,t.jsxs)(a.ol,{start:"2",children:["\n",(0,t.jsx)(a.li,{children:"The user is then asked to enter credentials for a PostgreSQL source."}),"\n"]}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/3.1-2_source.png",alt:"Willow's form for connecting to a source database."})}),"\n",(0,t.jsxs)(a.ol,{start:"3",children:["\n",(0,t.jsx)(a.li,{children:"Once a connection to the source database is established, the user can view and select the tables and columns to be captured. The user must also provide a name for the source connector."}),"\n"]}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/3.1-3_select_data.png",alt:"Willow's form for selecting which data should be replicated from the source database."})}),"\n",(0,t.jsxs)(a.ol,{start:"4",children:["\n",(0,t.jsx)(a.li,{children:"After data selection, users must enter the Redis credentials and verify the connection to the cache."}),"\n"]}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/3.1-4_sink.png",alt:"Willow's form for selecting which data should be replicated from the source database."})}),"\n",(0,t.jsxs)(a.ol,{start:"5",children:["\n",(0,t.jsx)(a.li,{children:"Once the sink connection is verified, users must provide a name for the sink connection. This completes the pipeline setup."}),"\n"]}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/3.1-5_sink_name.png",alt:"Willow's form for selecting which data should be replicated from the source database."})}),"\n",(0,t.jsx)(a.h2,{id:"4---implementation",children:"4 - Implementation"}),"\n",(0,t.jsx)(a.p,{children:"Willow leverages open-source technologies - Debezium, Apache Kafka, Apache Zookeeper, Kafka Connect, PostgreSQL - to create CDC pipelines and ensure that changes in source databases are updated in sink caches in near real-time. This section introduces Willow's components and explains their roles within Willow\u2019s architecture."}),"\n",(0,t.jsx)(a.h3,{id:"41---debezium",children:"4.1 - Debezium"}),"\n",(0,t.jsx)(a.p,{children:"When we first sought to address the cache consistency problem, we prioritized open-source CDC tools that are widely used, are well documented, and implement log-based CDC. This criteria is how we landed upon Debezium."}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/4.1-database_debezium.svg",alt:"Image showing the high level relationship between a database and Debezium. Data flows from the database to Debezium."}),(0,t.jsx)("figcaption",{children:"High level relationship between database and Debezium"})]}),"\n",(0,t.jsxs)(a.p,{children:["At the heart of Willow lies Debezium, an open-source distributed platform for change data capture. Debezium monitors databases\u2019 transaction logs and captures row-level changes for operations such as ",(0,t.jsx)(a.code,{children:"INSERT"}),", ",(0,t.jsx)(a.code,{children:"UPDATE"}),", and ",(0,t.jsx)(a.code,{children:"DELETE"}),". It produces events for such changes, and pushes those events downstream to an event-consuming process."]}),"\n",(0,t.jsxs)(a.p,{children:["Previously, when we defined change data capture, we outlined three methods for implementing CDC. We highlighted that log-based CDC is arguably superior to the other two approaches, but a downside is that transaction logs varied in format across database management systems. For example, MySQL\u2019s ",(0,t.jsx)(a.em,{children:"binlog"})," is different from PostgreSQL\u2019s ",(0,t.jsx)(a.em,{children:"write-ahead log"}),", despite their similar purposes."]}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/4.1-log_names.svg",alt:"Table showing what term various database management systems use to refer to their transaction log."}),(0,t.jsx)("figcaption",{children:"DBMSs and their respective transaction log names"})]}),"\n",(0,t.jsx)(a.p,{children:"Debezium addresses this lack of standardization by providing connectors. These connectors read from the database and produce events that have similar structure, regardless of the type of source database. Supported databases include PostgreSQL, MongoDB, and MySQL, among others. In other words, Debezium abstracts away the complexity of dealing with unstandardized transaction logs, and provides standardization for changes to be captured and handled downstream."}),"\n",(0,t.jsx)(a.p,{children:"Debezium, to the best of our knowledge, is the only open-source CDC tool that can capture from a variety of databases. Debezium's in-depth documentation and wide usage make it a reasonable tool for Willow to use."}),"\n",(0,t.jsx)(a.h3,{id:"42---apache-kafka",children:"4.2 - Apache Kafka"}),"\n",(0,t.jsx)(a.p,{children:"Apache Kafka is an open-source, distributed event streaming platform."}),"\n",(0,t.jsxs)(a.p,{children:["While Kafka is a broad topic and has many moving parts, the core workflow is simple: ",(0,t.jsx)(a.em,{children:"producers"})," send messages to Kafka brokers, which can then be processed by ",(0,t.jsx)(a.em,{children:"consumers"}),". Within Willow, Kafka is a message broker that stores streams of Debezium events."]}),"\n",(0,t.jsxs)(a.p,{children:["At its core, Apache Kafka consists of append-only logs, where messages are stored in sequential order. Kafka calls these logs ",(0,t.jsx)(a.strong,{children:"topics"}),". Topics are where ",(0,t.jsx)(a.strong,{children:"events"})," - records of a state change - are stored."]}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/4.2-kafka_log.svg",alt:"Image demonstrating that a Kafka topic is essentially a log. The topic contains four events. The first event in the log is at position 0. The next event to be placed in the topic will be appended to the end at position 4."})}),"\n",(0,t.jsxs)(a.p,{children:["A single ",(0,t.jsx)(a.strong,{children:"broker"}),", or individual Kafka server, is responsible for storing and managing one or more topics. A group of brokers, called a ",(0,t.jsx)(a.strong,{children:"cluster"}),", work together to handle incoming events. Brokers within a cluster can be ",(0,t.jsx)(a.strong,{children:"distributed"})," across a network, and clustered brokers can replicate each others\u2019 topics to provide data backups."]}),"\n",(0,t.jsx)(a.p,{children:"Because Kafka can be distributed across different servers and enable data replication, it is highly scalable and fault tolerant. Furthermore, its core data structure, an append-only log, enables fast reads and writes."}),"\n",(0,t.jsxs)(a.p,{children:["Apache Kafka clusters can be handled by ",(0,t.jsx)(a.strong,{children:"Apache Zookeeper"}),", which manages metadata on Kafka\u2019s components. Zookeeper functions as a centralized controller."]}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/4.2-debezium_to_kafka.png",alt:"Image demonstrating the high level relationship between Debezium and Kafka. Data flows from Debezium to Kafka. Kafka stores the data in topics. Data in topics is then read by consumers."}),(0,t.jsx)("figcaption",{children:"High level relationship between Debezium and Kafka"})]}),"\n",(0,t.jsx)(a.p,{children:"We chose Apache Kafka for a few reasons. The first is that Debezium is natively built on top of it; there is wide support and documentation for streaming Debezium events to Kafka, and how such events can be processed by downstream consumers. The second, related reason is a tool called Kafka Connect. Kafka Connect is a framework that allows one to set up, update, and tear down source and sink connectors that use Kafka as a message broker."}),"\n",(0,t.jsx)(a.p,{children:"Debezium has three deployment methods: Debezium Engine, Debezium Server, and deployment through Kafka Connect Debezium Server and Debezium Engine largely lack the ease of use provided by Kafka Connect\u2019s REST API, and would require specifying a message broker. The third deployment method - deployment via Kafka Connect - streams changes directly to Apache Kafka. It provides an easy to use REST API for configuring and setting up connectors to Apache Kafka. Using Kafka Connect allows us to leverage Apache Kafka\u2019s advantages, which includes persistence of records to disk in a way that is optimized for speed and efficiency."}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/4.2-database_connect_kafka.svg",alt:"Image demonstrating that Kafka Connect creates a Debezium connector between a database and Kafka."}),(0,t.jsx)("figcaption",{children:"Debezium\u2019s PostgreSQL source connector can be deployed via Kafka Connect, creating a connection between a database and Kafkaka"})]}),"\n",(0,t.jsx)(a.p,{children:"Apache Kafka\u2019s robust features, along with the tools that surround it - most notably, Kafka Connect - make it a logical message broker to be used with Debezium."}),"\n",(0,t.jsx)(a.h3,{id:"43---willow-adapter",children:"4.3 - Willow Adapter"}),"\n",(0,t.jsxs)(a.p,{children:["While Kafka enabled us to have a reliable streaming platform to publish database change events captured by Debezium, we still needed a way to consume those events and transform them into a suitable format for a cache. Initially, we researched a Redis Kafka Connect connector that consumes events from a Kafka topic and writes to a Redis cache. This connector converts events into Redis data types and generates a unique Redis key for each row. When assessing this connector, we were able to reflect ",(0,t.jsx)(a.code,{children:"CREATE"})," and ",(0,t.jsx)(a.code,{children:"UPDATE"})," changes to the cache, but ",(0,t.jsx)(a.code,{children:"DELETE"})," changes were not reflected in the cache. Also, additional Kafka Connect metadata was inserted into the cache, which was undesirable since we only want database rows to be reflected in the cache."]}),"\n",(0,t.jsx)(a.p,{children:"The Redis Kafka Connector connector was not optimal for Willow's needs, so we chose to implement our own custom Redis connector - a Willow Adapter for Kafka built with NodeJS. The Willow Adapter provides similar functionality to the Redis Kafka Connect connector, but it handles database level deletes appropriately by removing the associated data from the cache. The Willow Adapter also only inserts database row information - no Kafka Connect metadata is inserted. To consume messages from a Kafka topic, the KafkaJS npm package was used. To process those messages and update the cache, a custom class was created that leverages the Redis npm package."}),"\n",(0,t.jsx)(a.p,{children:"In order to provide a user-friendly UI for building a CDC pipeline, the Willow Adapter both provides a React application and acts as the REST API for Willow\u2019s UI, simplifying setup and teardown of each pipeline."}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/4.3-nodejs_v2.svg",alt:"Image demonstrating that data flows from Kafka into the Willow Adapter then into the sink cache."})}),"\n",(0,t.jsx)(a.h3,{id:"44---postgresql",children:"4.4 - PostgreSQL"}),"\n",(0,t.jsx)(a.p,{children:"The final component of Willow's architecture is a PostgreSQL database. PostgreSQL is an open-source, relational database management system (RDBMS) that stores structured data in tables. Data within PostgreSQL can be retrieved by using Structured Query Language (SQL) queries."}),"\n",(0,t.jsx)(a.p,{children:"An RDBMS works well when data follows a well-defined format and associations exist between different data entities. Willow uses PostgreSQL for its RDBMS, which is appropriate since associations exist among Willow's various entities; notably, each pipeline is associated with a source and sink. Configuration details for sources and sinks also follow a well-defined format, aligning with the type of structured data PostgreSQL excels at persisting. By storing pipeline information within a PostgreSQL database, Willow can redisplay existing pipeline information in its UI."}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/4.4-postgres.svg",alt:"Image demonstrating that a PostgreSQL database persists connection data for Willow."})}),"\n",(0,t.jsx)(a.h2,{id:"5---architecture",children:"5 - Architecture"}),"\n",(0,t.jsx)(a.p,{children:"As shown, Willow\u2019s pipeline is built upon various open-source tools. The components can be summarized as follows:"}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/5-table.svg",alt:"Table summarizing Willow's individual architectural components."})}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/5-architecture.svg",alt:"Image showing Willow's architecture. All components mentioned in section 4 are included."})}),"\n",(0,t.jsx)(a.p,{children:"To minimize potential configuration issues with installing Willow, we use Docker. Containerizing Willow also makes sense from a long-term perspective; Docker supports various orchestration tools like Kubernetes or Docker Swarm that allows users to manage and scale containers. In other words, Docker is not only portable and consistent across various environments, but it is also horizontally scalable. Its ease of use and portability make it an important piece of Willow."}),"\n",(0,t.jsx)(a.p,{children:"The final architecture is as follows:"}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/5-architecture_with_docker.svg",alt:"Image showing Willow's architecture including Docker."})}),"\n",(0,t.jsx)(a.h2,{id:"6---challenges",children:"6 - Challenges"}),"\n",(0,t.jsx)(a.p,{children:"Willow's development encountered two main technical challenges: Debezium configuration and event transformation."}),"\n",(0,t.jsx)(a.h3,{id:"61---debezium-configuration",children:"6.1 - Debezium Configuration"}),"\n",(0,t.jsx)(a.h4,{id:"611---multiple-pipelines-sharing-a-replication-slot",children:"6.1.1 - Multiple Pipelines Sharing a Replication Slot"}),"\n",(0,t.jsx)(a.p,{children:"One of the challenges was centered around multiple pipelines sharing a replication slot. A replication slot is a PostgreSQL feature that keeps track of the last-read entry in the write-ahead log - PostgreSQL\u2019s transaction log - for a specific consumer. For Willow, a consumer is equivalent to a pipeline."}),"\n",(0,t.jsx)(a.p,{children:"In an early version, Willow reused a single replication slot for every pipeline connected to a PostgreSQL server, but only a single pipeline received changes and the remaining pipelines received none. This occurred since all pipelines for a single PostgreSQL server shared a replication slot and only one pipeline can consume from a replication slot at a time. However, pipelines connected to the same PostgreSQL server should be considered independent entities that consume all relevant changes."}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/6.1.1-Multiple Pipelines Sharing Replication Slot A.gif",alt:"Animation showing two pipelines sharing a replication slot. Data flows from the database's write-ahead log and into the replication slot. Data flows from the replication slot to only one of the connected pipelines. The other pipeline does not receive any data."})}),"\n",(0,t.jsx)(a.p,{children:"In order to ensure multiple pipelines using the same PostgreSQL server receive every change, Willow creates a unique replication slot for each pipeline. By doing so, Willow enables each pipeline to concurrently and separately read from the write-ahead log."}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/6.1.1-Multiple Pipelines Separate Replication Slots.gif",alt:"Animation showing two pipelines with separate replication slots. Data flows from the database's write-ahead log and into the replication slots. Data flows from the replication slots to their associated pipelines."})}),"\n",(0,t.jsx)(a.p,{children:"A tradeoff to this approach is that a new replication slot is created in the PostgreSQL server each time a pipeline is generated. This can result in multiple replication slots, creating opportunities for inactive slots when associated pipelines are torn down. Inactive replication slots force PostgreSQL to indefinitely retain all write-ahead log files unread by the associated pipeline, filling up disk space. Database administrators must carefully monitor and purge any inactive replication slots to avoid unnecessary write-ahead log retention."}),"\n",(0,t.jsx)(a.h4,{id:"612---working-with-minimum-privileged-user",children:"6.1.2 - Working with Minimum Privileged User"}),"\n",(0,t.jsxs)(a.p,{children:["The second challenge was ensuring source connectors are successfully created when Willow is provided a database user that does not have ",(0,t.jsx)(a.code,{children:"SUPERUSER"})," privileges. A PostgreSQL ",(0,t.jsx)(a.code,{children:"SUPERUSER"})," bypasses all permission checks and accesses everything in the server. A minimum privileged user, on the other hand, only has sufficient permissions for Willow to create a source connector and replicate specific tables within the PostgreSQL server. These privileges are ",(0,t.jsx)(a.code,{children:"REPLICATION"}),", ",(0,t.jsx)(a.code,{children:"LOGIN"}),", database level ",(0,t.jsx)(a.code,{children:"CREATE"}),", and table level ",(0,t.jsx)(a.code,{children:"SELECT"})," privileges."]}),"\n",(0,t.jsxs)(a.p,{children:["Debezium successfully creates a source connector to a PostgreSQL server when the provided PostgreSQL user either is a ",(0,t.jsx)(a.code,{children:"SUPERUSER"})," or a minimum privileged user. Willow follows the principle of least privilege - the idea of providing users and programs only the minimum level of access needed to perform their responsibilities - by allowing end users to provide a minimum privileged PostgreSQL user."]}),"\n",(0,t.jsxs)(a.p,{children:["Debezium uses the minimum privileged user to create a publication. A ",(0,t.jsx)(a.strong,{children:"publication"})," is a database-scoped sequential list of ",(0,t.jsx)(a.code,{children:"INSERT"}),", ",(0,t.jsx)(a.code,{children:"UPDATE"}),", ",(0,t.jsx)(a.code,{children:"DELETE"}),", and ",(0,t.jsx)(a.code,{children:"TRUNCATE"})," changes for selected tables and are how pipelines receive data for what changes occur in those tables."]}),"\n",(0,t.jsxs)(a.p,{children:["Initially, Willow worked well when provided a ",(0,t.jsx)(a.code,{children:"SUPERUSER"})," but failed when given a minimum privileged user. The core issue came down to how Debezium creates publications in the source database and how Willow was configuring Debezium with a minimum privileged user."]}),"\n",(0,t.jsxs)(a.p,{children:["Initially, Willow used Debezium's ",(0,t.jsx)(a.code,{children:"table.exclude.list"})," setting to specify which tables should not be replicated. By process of elimination, Debezium would then attempt to replicate all of the non-excluded tables in the database. However, if private tables exist in the database that the minimum privileged user does not have access to, these tables would not be visible when Willow queried the database to determine what tables exist, and those non-visible tables would not be listed in ",(0,t.jsx)(a.code,{children:"table.exclude.list"}),". As a result, Debezium\u2019s publication creation would fail since the publication contained tables inaccessible to the minimum privileged user."]}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/6.1.2-Minimum Privileged User_1.gif",alt:"Animation showing Debezium attempting to create a publication with a minimum privileged user. Since the table.exclude.list setting is used, Debezium's publication creation attempt fails."})}),"\n",(0,t.jsxs)(a.p,{children:["This issue was resolved by using Debezium's ",(0,t.jsx)(a.code,{children:"table.include.list"})," instead of its exclude counterpart. This strategy of white-listing instead of black-listing tells Debezium exactly which tables to include in the publication, preventing Debezium from including tables inaccessible to the minimum privileged user."]}),"\n",(0,t.jsx)("figure",{children:(0,t.jsx)("img",{src:"/img/case-study/6.1.2-Minimum Privileged User_2.gif",alt:"Animation showing Debezium attempting to create a publication with a minimum privileged user. Since the table.include.list setting is used, Debezium's publication creation attempt succeeds."})}),"\n",(0,t.jsx)(a.h3,{id:"62---event-transformation",children:"6.2 - Event Transformation"}),"\n",(0,t.jsx)(a.p,{children:"Events generated by Debezium take the shape of a key-value pair, representing an individual table\u2019s row change. The key contains information about the row's primary key value, and the value contains information about the type of change and the row's updated values."}),"\n",(0,t.jsxs)(a.p,{children:["Willow uses a combination of the event\u2019s key and value to determine the Redis key, which follows the format ",(0,t.jsx)(a.code,{children:"database.table.primarykey"}),"."]}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/6.2-transformation_process.svg",alt:"Image demonstrating the transformation process to convert database write-ahead log entries into Redis key-value pairs."}),(0,t.jsx)("figcaption",{children:"High-level proces of write-ahead log entries transforming into Redis key-value pairs."})]}),"\n",(0,t.jsx)(a.p,{children:"When transforming events into Redis key-value pairs, Willow handles a few edge cases: tombstone events, tables without primary keys, and tables with composite primary keys."}),"\n",(0,t.jsx)(a.h4,{id:"621---tombstone-events",children:"6.2.1 - Tombstone Events"}),"\n",(0,t.jsxs)(a.p,{children:["When a row is deleted, Debezium generates two events. The first is a ",(0,t.jsx)(a.code,{children:"DELETE"})," event containing information about the deleted row. The second, called a tombstone event, has a ",(0,t.jsx)(a.code,{children:"key"})," property containing the deleted row\u2019s primary key and a ",(0,t.jsx)(a.code,{children:"value"})," property with a ",(0,t.jsx)(a.code,{children:"null"})," value."]}),"\n",(0,t.jsx)(a.p,{children:"Tombstone events are used by Apache Kafka to remove all previous records related to that row in a process called log compaction. Log compaction helps Kafka reduce the size of each topic while still retaining enough information to replicate the table's current state."}),"\n",(0,t.jsx)(a.p,{children:"Since tombstone events are used for Kafka\u2019s log compaction, the Willow Adapter ignores them when transforming Debezium events to Redis key-value pairs."}),"\n",(0,t.jsx)(a.h4,{id:"622---no-primary-key",children:"6.2.2 - No Primary Key"}),"\n",(0,t.jsxs)(a.p,{children:["A table's primary key is a column that contains a unique, not ",(0,t.jsx)(a.code,{children:"null"})," value and uniquely identifies a single row. For tables with no primary keys, events have ",(0,t.jsx)(a.code,{children:"null"})," keys."]}),"\n",(0,t.jsx)(a.p,{children:"In this situation, Willow is opinionated and prevents tables without primary keys from being replicated. Willow\u2019s UI does not show tables without a primary key as an option when selecting tables to replicate. Without a primary key, it is difficult to determine which Redis key-value pair should be created, updated or deleted."}),"\n",(0,t.jsx)(a.p,{children:"The tradeoff of this decision is that Willow cannot be used for tables without a primary key. While these types of tables are sometimes used for containing message logs, their usage is infrequent when following good database design, and their contents are not typically desirable to replicate in a cache."}),"\n",(0,t.jsx)(a.h4,{id:"623---composite-primary-key",children:"6.2.3 - Composite Primary Key"}),"\n",(0,t.jsx)(a.p,{children:"Instead of using a single column as the primary key, a table can use the combination of two or more columns to uniquely identify each row. This combination of columns is called a composite primary key."}),"\n",(0,t.jsxs)(a.p,{children:["In the visual below, the combination of the ",(0,t.jsx)(a.code,{children:"order_id"})," and ",(0,t.jsx)(a.code,{children:"payment_id"})," columns is the composite primary key for the Payments table."]}),"\n",(0,t.jsxs)("figure",{children:[(0,t.jsx)("img",{src:"/img/case-study/6.2.3-composite_pkey.svg",alt:"Database table that has a composite primary key. The two columns, order_id and payment_id, that are used in the composite primary key are boxed in green."}),(0,t.jsxs)("figcaption",{children:["The Payments table has a comopsite primary key comprised of the ",(0,t.jsx)(a.code,{children:"order_id"})," and ",(0,t.jsx)(a.code,{children:"payment_id"})," columns (boxed in green)."]})]}),"\n",(0,t.jsx)(a.p,{children:"For tables with a composite primary key, event keys contain information about all column values contributing to the row\u2019s composite key."}),"\n",(0,t.jsxs)(a.p,{children:["Willow supports usage of composite primary keys by joining the individual values with a dot when creating the key for Redis' key-value pair, such as ",(0,t.jsx)(a.code,{children:"database.table.keyvalue1.keyvalue2"}),"."]}),"\n",(0,t.jsx)(a.h2,{id:"7---conclusion-and-future-work",children:"7 - Conclusion and Future Work"}),"\n",(0,t.jsx)(a.p,{children:"To conclude, Willow is an open-source, self-hosted event-driven framework with a specific use case. It utilizes log-based change data capture to build event streaming pipelines, capturing row-level changes from databases and updating caches in near real-time. It solves a specific form of the cache consistency problem and bypasses various existing solutions, such as TTL and polling. With its simple and intuitive UI, as well as the ability to select which tables and columns to capture, Willow fills a niche in the cache invalidation solution space."}),"\n",(0,t.jsx)(a.p,{children:"While we are happy with Willow, there is room for improvement. The following are areas where we would like to expand upon in the future."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"Decoupling Source and Sink Connectors"})}),"\n",(0,t.jsx)(a.p,{children:"In our current architecture, pipelines have a one-to-one relationship with a source database and a sink cache. That is, when a user wants to create a pipeline, they must enter information for some source database and sink cache, even if that information has already been entered for another Willow pipeline. In the enterprise solutions we\u2019ve seen, source connector information can be registered without only being  used for a single pipeline; one can specify a source connector that can then be used for multiple pipelines. The same holds for sink connectors."}),"\n",(0,t.jsx)(a.p,{children:"Decoupling source and sink connectors would also enable Willow to stream changes to different caches without needing to establish new pipelines. Currently for each Willow pipeline, there is one source connector and one sink connector. In certain situations, users may want to stream changes from one source database to multiple Redis caches. To do so, for each additional cache, a user would need to create a new pipeline. However, this is not an ideal design, as each additional pipeline creates redundant Kafka topics, taking up additional disk space. By decoupling source and sink connectors, Willow users could simply tie multiple Redis sinks to the same source database in one pipeline. This would grant users more flexibility and be a more efficient use of resources."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"Deployment and Management"})}),"\n",(0,t.jsx)(a.p,{children:"Another feature we would like to enable is automatic deployment and management across distributed servers. Currently, Willow operates as a multi-container application on one server. While users can deploy Willow on multiple servers themselves, it would be valuable to have Willow provide that capability by default. This option would make Willow more easily available and fault-tolerant in case a container breaks. We would look into container orchestration tools like Kubernetes or Docker Swarm to implement this feature."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"Observability"})}),"\n",(0,t.jsx)(a.p,{children:"Currently, there are no observability metrics for monitoring the health and status of Willow\u2019s pipelines. Apache Kafka, Apache Zookeeper, and Kafka Connect all enable Java Management Extensions (JMX), which are technologies that monitor and manage Java applications. Setting up JMX would provide metrics on various groups within the Willow architecture, including Kafka brokers, Zookeeper controllers, and Kafka consumers. Furthermore, in addition to JMX metrics, Debezium connectors provide ways to set up additional monitoring. These primarily provide snapshot and streaming metrics. Adding observability features would allow Willow\u2019s end-users to be better informed and more easily identify issues."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"Other ideas"})}),"\n",(0,t.jsx)(a.p,{children:"Other features we would like to add include:"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"The ability to connect more types of sources and sinks. Currently, Willow only captures changes from PostgreSQL databases into Redis caches."}),"\n",(0,t.jsx)(a.li,{children:"The option to encrypt certain steps in the pipeline. For example, while we currently set up Debezium to prefer to use an encrypted connection to the source database, if no certifications are provided, Debezium can default to an unencrypted connection."}),"\n",(0,t.jsx)(a.li,{children:"Adding more Redis types. Currently, rows are stored as Redis JSON types within the cache. While we believe this offers the most flexibility, allowing users to store as other types (Redis Hashes, Strings) can be beneficial."}),"\n"]}),"\n",(0,t.jsx)(a.h3,{id:"8---references",children:"8 - References"}),"\n",(0,t.jsxs)(a.ol,{children:["\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://www.gigaspaces.com/blog/amazon-found-every-100ms-of-latency-cost-them-1-in-sales",children:"Amazon Found Every 100ms of Latency Cost them 1% in Sales"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://www.thinkwithgoogle.com/intl/en-emea/marketing-strategies/app-and-mobile/mobile-page-speed-data/",children:"Google Study on Website Performance"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://redis.com/blog/how-slow-queries-hurt-your-business/",children:"How Slow Queries Hurt Your Business"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://medium.com/@mmoshikoo/cache-strategies-996e91c80303",children:"Cache Strategies"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://www.geeksforgeeks.org/cache-invalidation-and-the-methods-to-invalidate-cache/",children:"Cache Invalidation"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://www.qlik.com/us/change-data-capture/cdc-change-data-capture",children:"What is Change Data Capture"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://www.pubnub.com/blog/how-fast-is-realtime-human-perception-and-technology/",children:"How Fast is Real-Time?"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://www.confluent.io/learn/change-data-capture/",children:"Change Data Capture Methods"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://debezium.io/",children:"Debezium"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://kafka.apache.org/",children:"Apache Kafka"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://www.postgresql.org/",children:"PostgreSQL"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://www.postgresql.org/docs/10/logicaldecoding-explanation.html#LOGICALDECODING-REPLICATION-SLOTS",children:"PostgreSQL Replication Slots"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://debezium.io/documentation/reference/2.5/connectors/postgresql.html#postgresql-permissions",children:"Minimum Privileged User Permissions"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://debezium.io/documentation/reference/2.5/connectors/postgresql.html#postgresql-delete-events",children:"Tombstone Events"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://www.postgresql.org/docs/current/ddl-constraints.html#DDL-CONSTRAINTS-PRIMARY-KEYS",children:"Primary Keys"})}),"\n",(0,t.jsx)(a.li,{children:(0,t.jsx)(a.a,{href:"https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-replica-identity",children:"Usage of Tables with No Primary Keys"})}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,a,i)=>{i.d(a,{R:()=>o,x:()=>r});var t=i(6540);const s={},n=t.createContext(s);function o(e){const a=t.useContext(n);return t.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function r(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(n.Provider,{value:a},e.children)}}}]);