<!doctype html>
<html lang="en" dir="ltr" class="mdx-wrapper mdx-page plugin-pages plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.2.1">
<title data-rh="true">Case Study | Willow</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://willow-cdc.github.io/img/willow_transparent"><meta data-rh="true" name="twitter:image" content="https://willow-cdc.github.io/img/willow_transparent"><meta data-rh="true" property="og:url" content="https://willow-cdc.github.io/case-study"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Case Study | Willow"><meta data-rh="true" name="description" content="Willow Case Study"><meta data-rh="true" property="og:description" content="Willow Case Study"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://willow-cdc.github.io/case-study"><link data-rh="true" rel="alternate" href="https://willow-cdc.github.io/case-study" hreflang="en"><link data-rh="true" rel="alternate" href="https://willow-cdc.github.io/case-study" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.43161526.css">
<script src="/assets/js/runtime~main.6ef425bd.js" defer="defer"></script>
<script src="/assets/js/main.fe8644e8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Willow Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="Willow Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Willow</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/case-study">Case Study</a><a class="navbar__item navbar__link" href="/docs/Directions">Docs</a><a class="navbar__item navbar__link" href="/#team">Team</a><a href="https://github.com/willow-cdc" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><main class="container container--fluid margin-vert--lg"><div class="row mdxPageWrapper_j9I6"><div class="col col--8"><article><h1>Willow Case Study</h1>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="0---introduction">0 - Introduction<a href="#0---introduction" class="hash-link" aria-label="Direct link to 0 - Introduction" title="Direct link to 0 - Introduction">​</a></h2>
<p>Willow is an open-source, self-hosted framework for building log-based change data capture pipelines that update caches in near real-time based on changes in a database.</p>
<p>Willow abstracts away the complexity of setting up and configuring open source tools like Debezium and Apache Kafka. Utilizing log-based change data capture, Willow non-invasively monitors changes in a user&#x27;s PostgreSQL database and reflects those row-level changes in a user&#x27;s Redis cache. Without requiring in-depth technical knowledge and expertise, data pipelines can be created or deleted using Willow&#x27;s web user interface, simplifying setup and teardown.</p>
<p>This case study provides background on caching and its tradeoffs, explains change data capture, and discusses why developers would use it to keep a cache consistent. Next, Willow&#x27;s implementation and architecture are explored, and challenges encountered when creating Willow and how these were addressed is explained. Finally, Willow&#x27;s roadmap for the future is outlined.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1---background">1 - Background<a href="#1---background" class="hash-link" aria-label="Direct link to 1 - Background" title="Direct link to 1 - Background">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="11---general-problem-domain">1.1 - General Problem Domain<a href="#11---general-problem-domain" class="hash-link" aria-label="Direct link to 1.1 - General Problem Domain" title="Direct link to 1.1 - General Problem Domain">​</a></h3>
<p>A web application&#x27;s response time significantly influences user experience and company profitability. An Amazon study found that every 100ms of latency dropped sales by 1%. Google&#x27;s research found that a 100ms improvement in mobile site speeds increased how many visitors made purchases by up to 10%. Web applications are growing increasingly complex and distributed, forcing developers to find ways to improve application speed and response time.</p>
<p>One source of slowdown is an application&#x27;s database. Database queries can become a limiting factor for an application’s response time for various reasons, including when multiple applications are simultaneously accessing the database or when unoptimized queries block other queries from being executed. A common way to increase read response times is to utilize a cache.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="12---caching">1.2 - Caching<a href="#12---caching" class="hash-link" aria-label="Direct link to 1.2 - Caching" title="Direct link to 1.2 - Caching">​</a></h3>
<p>Caching is a strategy to improve web application performance by taking demand off of source databases and providing speedy access to data. With caching, frequently accessed data is stored in a temporary location. This is usually an in-memory form of storage, which means data can be accessed faster than persistent, disk-based storage. By utilizing caching, an application checks the cache for data before requesting it from the database. This can reduce read demands on the source, taking pressure off of the database and increasing the speed of the application overall.</p>
<p>However, there are tradeoffs when using a cache. One major challenge is keeping data in the cache up-to-date and consistent with the source of truth. This issue is known as cache consistency.</p>
<p>Caching is a viable strategy when there is only one application making changes to the source database. When the architecture is simple, caches can improve performance while avoiding cache inconsistency. This is the case with many types of caching strategies: read-through, write-through, and even cache-aside. Each of these strategies positions the cache either beside the application or between the application and the database. With read-through and cache-aside approaches, the cache is checked for the requested data before the application checks the database. If the data is not found in the cache, this is called a cache miss, and requires the application or cache to check the database for the missing data. Once the queried data is retrieved, it is then stored in the cache for future requests. A write-through cache mirrors what’s in the database, as any writes to the database are first sent to the cache, written there, and then forwarded to the database.</p>
<figure><img src="/img/case-study/1.2-Cache-Aside.svg" alt="A cache aside strategy"><figcaption>Cache aside</figcaption></figure>
<figure><img src="/img/case-study/1.2-Cache-Write-through.svg" alt="A write-through cache strategy"><figcaption>Write-through</figcaption></figure>
<figure><img src="/img/case-study/1.2-Cache-Read-Through.svg" alt="A read-through cache strategy"><figcaption>Read-through</figcaption></figure>
<p>Cache inconsistency becomes a problem when another application or component makes updates to the source database. Because the cache is unaware of these changes, it is possible for the cache to become inconsistent with the source and serve data that is no longer in sync with the source database. This out-of-sync data is referred to as <strong>stale data</strong>.</p>
<figure><img src="/img/case-study/1.2-Complex-Architecture.gif" alt="Cache inconsistency occurs in complex architectures"><figcaption>Cache inconsistency occurs in complex architectures</figcaption></figure>
<p>There are cache invalidation strategies, such as Time-To-Live (TTL) and polling, that attempt to mitigate this problem.</p>
<p>TTL is a period of time that a value should exist in a cache before being discarded. This approach aims to minimize the presence of stale data. After the data expires and the application’s query to the cache results in a cache miss, the application queries the source database and repopulates the cache. This process incurs additional resource costs and increases demand on the source, especially for expired data that is not stale. This demand is exacerbated when the expiration time is made shorter, and checks on the database for updated data increase. Likewise, longer TTL periods increase the likelihood of stale data, which can be unacceptable for applications requiring timely data accuracy. While TTL can be fairly straightforward to implement, choosing the TTL value that reduces stale data and minimizes database queries is difficult.</p>
<figure><img src="/img/case-study/1.2-TTL Too Short.gif" alt="Short TTL can result in unnecessary network requests"><figcaption>Short TTL can result in unnecessary network requests</figcaption></figure>
<figure><img src="/img/case-study/1.2-TTL Too Long.gif" alt="Long TTL can result in retrieving stale data"><figcaption>Long TTL can result in retrieving stale data</figcaption></figure>
<p>An alternative cache invalidation strategy, polling is when the cache or application periodically checks if the cache’s data is inconsistent with the source database, updating any inconsistent data. Polling typically happens on fixed intervals, such as every 10 seconds, to ensure data consistency. While polling can decrease the time that data remains stale, it also puts extra demand on the source database by running frequent queries.</p>
<figure><img src="/img/case-study/1.2-polling.svg" alt="Polling places additional strain on the database"><figcaption>Polling places additional strain on the database</figcaption></figure>
<p>These cache invalidation strategies - TTL and polling - consist of seeking out and pulling changes from the database. For systems that don’t require constant data consistency and can endure brief moments of stale data, these strategies may be appropriate. However, TTL and polling are not appropriate for systems that require fresh data in near real-time. In the next section, we examine change data capture, a strategy that is not only more appropriate for these types of systems, but can put less demand on the source database.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="13---change-data-capture">1.3 - Change Data Capture<a href="#13---change-data-capture" class="hash-link" aria-label="Direct link to 1.3 - Change Data Capture" title="Direct link to 1.3 - Change Data Capture">​</a></h3>
<p>Change data capture has many use cases, including keeping caches in sync with databases.</p>
<p>Change data capture (CDC) refers to the process of identifying and capturing changes in a <strong>source</strong> - a system that provides data - and then delivering those changes in near real-time to a <strong>sink</strong> - a system that receives data. Near real-time (or “soft real-time”) refers to the processing of data where systems can tolerate slight delays from a few seconds to a few minutes. This is where most networked communication lies and is opposed to “hard real-time” systems, where data processing is benchmarked against 250 milliseconds (the average human response time) of delay.</p>
<p>CDC can be implemented through a series of different methods, each having their own benefits and tradeoffs.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="131-timestamp">1.3.1 Timestamp<a href="#131-timestamp" class="hash-link" aria-label="Direct link to 1.3.1 Timestamp" title="Direct link to 1.3.1 Timestamp">​</a></h4>
<p>Timestamp-based CDC involves adding metadata columns (e.g., <code>created_at</code>, <code>updated_at</code>) to each database table. One limitation is the inability to perform permanent, hard deletes of rows. Metadata of all changes, including deletes, must persist in order to detect changes. Additionally, to keep the target system in sync, the database needs to be regularly queried for changes, putting additional overhead on the source system.</p>
<figure><img src="/img/case-study/1.3-timestamp.svg" alt="A database table demonstrating the timestamp-based CDC method. The updated at column is used to remember when a row was last updated."></figure>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="132-trigger">1.3.2 Trigger<a href="#132-trigger" class="hash-link" aria-label="Direct link to 1.3.2 Trigger" title="Direct link to 1.3.2 Trigger">​</a></h4>
<p>Trigger-based CDC relies on the database’s built-in functionality to invoke a custom function, or trigger, whenever a change is made to a table. Changes are usually stored in a different table within the same database called a shadow table or event table. A shadow table is essentially a time ordered changelog of all operations performed in the database, providing visibility for <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> changes.</p>
<p>While most database systems support triggers, this method has drawbacks. One drawback of trigger based CDC is every trigger requires an additional write operation to an event table. These additional writes impact database performance, especially at scale for write-heavy applications. Another limitation is the event table must be queried to propagate changes to any downstream processes.</p>
<figure><img src="/img/case-study/1.3-trigger.svg" alt="Two database tables demonstrating the trigger-based CDC method. Individual changes that occur in an Employees table are recorded in an Event table."></figure>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="133-log-based">1.3.3 Log-based<a href="#133-log-based" class="hash-link" aria-label="Direct link to 1.3.3 Log-based" title="Direct link to 1.3.3 Log-based">​</a></h4>
<p>Log-based CDC involves leveraging the database transaction log — a file that keeps a record of all changes made to the database — for capturing change events and delivering those to downstream processes. In log-based CDC, the database transaction log is asynchronously parsed to determine changes instead of formally querying the database. Hence, the log-based method is the least invasive out of the three methods, requiring the least additional computational overhead on the source database.</p>
<p>Although it offers superior performance and reduced latency, the log-based method comes with its own set of tradeoffs. Database transaction log formats are not standardized, so logs between database management systems can vary and vendors can change log formats in future releases. Custom code connectors are also needed in order to read from a transaction log. Additionally, these logs usually only store changes for a particular retention period.</p>
<figure><img src="/img/case-study/1.3-log.svg" alt="Image showing the general structure of the log-based CDC method. A CDC connector reads the database&#x27;s transaction log to determine changes."></figure>
<p>As mentioned, CDC can be used to build a low-latency data pipeline that propagates changes from a database to a cache. Out of the three, log-based CDC takes a non-invasive approach. Because it minimizes impact to database performance, we looked at log-based approaches when considering existing solutions.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2---existing-solutions">2 - Existing Solutions<a href="#2---existing-solutions" class="hash-link" aria-label="Direct link to 2 - Existing Solutions" title="Direct link to 2 - Existing Solutions">​</a></h2>
<p>Log-based CDC is popular among applications requiring up-to-date data in near real-time. Developers have several options for building a log-based CDC data pipeline to replicate data from a source database to a sink cache. Multiple enterprise solutions exist, as well as various open-source projects that can be combined for a custom DIY solution. When deciding which solution to pursue, developers should consider  scalability, ease of use and connector types available.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="21---enterprise-solutions">2.1 - Enterprise Solutions<a href="#21---enterprise-solutions" class="hash-link" aria-label="Direct link to 2.1 - Enterprise Solutions" title="Direct link to 2.1 - Enterprise Solutions">​</a></h3>
<p>Multiple enterprise solutions using CDC to replicate data from a source database to a sink cache are available. Prominent solutions include Redis Data Integration and Confluent. Both take care of managing the CDC pipeline and have a number of available source and sink connectors - applications capable of either extracting changes from a data source or replicating changes to a data destination. Redis Data Integration and Confluent are built on top of open-source tools (Debezium and Apache Kafka) and provide additional benefits, like a wide selection of source and sink connectors, architecture management, and built-in scalability.</p>
<figure><img src="/img/case-study/2.1-Confluent-Redis.svg" alt="Confluent and Redis logos."></figure>
<p>Enterprise solutions are a good fit for well-funded development teams that want a third-party to manage their architecture and hosting logistics. However, enterprise solutions come with tradeoffs, including vendor lock-in, recurring costs, and reduced infrastructure control. In particular, developers do not have control over how or when infrastructure is upgraded or maintained, which can result in service downtime.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="22---diy-solutions">2.2 - DIY Solutions<a href="#22---diy-solutions" class="hash-link" aria-label="Direct link to 2.2 - DIY Solutions" title="Direct link to 2.2 - DIY Solutions">​</a></h3>
<p>An alternative to enterprise solutions, DIY solutions can be built by leveraging open-source tools, like Debezium and Apache Kafka. These tools are open-source, provide a high level of data customization, and offer a wide number of community-maintained source and sink connectors. Customizations include but aren’t limited to filtering data, transforming data, aggregating data, and horizontally scaling CDC pipelines.</p>
<figure><img src="/img/case-study/2.2-Debezium-Kafka.svg" alt="Debezium and Kafka logos."></figure>
<p>Building a DIY solution using open-source tools is a good fit for development teams that prefer to manage their architecture, have a high level of control and customization in their CDC pipeline, and avoid recurring costs from enterprise providers. However, DIY solutions require significant and complex configuration of open-source tools, and developers must provision and manage appropriate infrastructure for the CDC pipeline. The time to learn, implement, and configure these technologies can slow down teams looking to quickly deploy a CDC pipeline.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3---introducing-willow">3 - Introducing Willow<a href="#3---introducing-willow" class="hash-link" aria-label="Direct link to 3 - Introducing Willow" title="Direct link to 3 - Introducing Willow">​</a></h2>
<p>Given the tradeoffs that accompany enterprise CDC solutions and the complexity of a DIY solution, our team identified a gap in the solution space. Willow was developed as an open source, user-friendly framework designed to maintain cache consistency by creating a near real-time CDC pipeline that monitors changes in a user&#x27;s PostgreSQL database and reflects row-level changes in a user&#x27;s Redis cache.</p>
<figure><img src="/img/case-study/3-comparison_table.svg" alt="Table comparing Willow against enterprise solutions and DIY solutions. Comparison criteria are no vendor lock in, easy to configure, infrastructure control, large number of connectors, and cost."></figure>
<p>Willow’s user-friendly UI abstracts away configuration complexities we encountered in the DIY solutions by guiding users through a set of forms to set up a pipeline. Once a pipeline is created, users can set up additional pipelines, view a list of existing pipelines and their configuration details, and delete a pipeline.</p>
<p>Willow can be deployed on users’ server of choice, allowing users to retain infrastructure control and avoiding vendor lock-in. While Willow only has a single source connector for PostgreSQL and a single sink connector for Redis, the simplicity of setting up and configuring these connectors into a CDC pipeline reduces overall deployment time and cost.</p>
<p>Willow pipelines connect a PostgreSQL database and a Redis cache, storing database rows in the Redis cache as JSON objects Updates made to the database are passed on to the cache in near real-time.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="31-demonstration">3.1 Demonstration<a href="#31-demonstration" class="hash-link" aria-label="Direct link to 3.1 Demonstration" title="Direct link to 3.1 Demonstration">​</a></h3>
<p>The best way to understand what Willow does is by seeing it in action. In the video below, a PostgreSQL terminal logged into the <code>willow</code> database is on the left and RedisInsight - a Redis GUI for visualizing a cache&#x27;s contents - is on the right. A Willow CDC pipeline connects the source PostgreSQL instance on the left to the sink Redis cache on the right.</p>
<p>Initially, the PostgreSQL <code>store</code> table and the Redis cache are empty. Once a row is inserted into <code>store</code>, Willow replicates the row in the cache. After refreshing RedisInsight, we can see that the row inserted into our PostgreSQL table has been replicated in our Redis cache.</p>
<figure><img src="/img/case-study/3.1-demo.gif" alt="A demonstration of Willow. On the left side of the screen is a PostgreSQL terminal. On the right side of the screen is a Redis cache shown in RedisInsight. An INSERT command is performed in the PostgreSQL terminal, and the inserted data automatically appears in the Redis cache."></figure>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="32---using-willow">3.2 - Using Willow<a href="#32---using-willow" class="hash-link" aria-label="Direct link to 3.2 - Using Willow" title="Direct link to 3.2 - Using Willow">​</a></h3>
<ol>
<li>Initially, users are greeted with a &quot;Welcome to Willow&quot; page, offering an invitation to create a CDC pipeline with a click of a button.</li>
</ol>
<figure><img src="/img/case-study/3.1-1_home.png" alt="Willow&#x27;s home page."></figure>
<ol start="2">
<li>The user is then asked to enter credentials for a PostgreSQL source.</li>
</ol>
<figure><img src="/img/case-study/3.1-2_source.png" alt="Willow&#x27;s form for connecting to a source database."></figure>
<ol start="3">
<li>Once a connection to the source database is established, the user can view and select the tables and columns to be captured. The user must also provide a name for the source connector.</li>
</ol>
<figure><img src="/img/case-study/3.1-3_select_data.png" alt="Willow&#x27;s form for selecting which data should be replicated from the source database."></figure>
<ol start="4">
<li>After data selection, users must enter the Redis credentials and verify the connection to the cache.</li>
</ol>
<figure><img src="/img/case-study/3.1-4_sink.png" alt="Willow&#x27;s form for selecting which data should be replicated from the source database."></figure>
<ol start="5">
<li>Once the sink connection is verified, users must provide a name for the sink connection. This completes the pipeline setup.</li>
</ol>
<figure><img src="/img/case-study/3.1-5_sink_name.png" alt="Willow&#x27;s form for selecting which data should be replicated from the source database."></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4---implementation">4 - Implementation<a href="#4---implementation" class="hash-link" aria-label="Direct link to 4 - Implementation" title="Direct link to 4 - Implementation">​</a></h2>
<p>Willow leverages open-source technologies - Debezium, Apache Kafka, Apache Zookeeper, Kafka Connect, PostgreSQL - to create CDC pipelines and ensure that changes in source databases are updated in sink caches in near real-time. This section introduces Willow&#x27;s components and explains their roles within Willow’s architecture.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="41---debezium">4.1 - Debezium<a href="#41---debezium" class="hash-link" aria-label="Direct link to 4.1 - Debezium" title="Direct link to 4.1 - Debezium">​</a></h3>
<p>When we first sought to address the cache consistency problem, we prioritized open-source CDC tools that are widely used, are well documented, and implement log-based CDC. This criteria is how we landed upon Debezium.</p>
<figure><img src="/img/case-study/4.1-database_debezium.svg" alt="Image showing the high level relationship between a database and Debezium. Data flows from the database to Debezium."><figcaption>High level relationship between database and Debezium</figcaption></figure>
<p>At the heart of Willow lies Debezium, an open-source distributed platform for change data capture. Debezium monitors databases’ transaction logs and captures row-level changes for operations such as <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code>. It produces events for such changes, and pushes those events downstream to an event-consuming process.</p>
<p>Previously, when we defined change data capture, we outlined three methods for implementing CDC. We highlighted that log-based CDC is arguably superior to the other two approaches, but a downside is that transaction logs varied in format across database management systems. For example, MySQL’s <em>binlog</em> is different from PostgreSQL’s <em>write-ahead log</em>, despite their similar purposes.</p>
<figure><img src="/img/case-study/4.1-log_names.svg" alt="Table showing what term various database management systems use to refer to their transaction log."><figcaption>DBMSs and their respective transaction log names</figcaption></figure>
<p>Debezium addresses this lack of standardization by providing connectors. These connectors read from the database and produce events that have similar structure, regardless of the type of source database. Supported databases include PostgreSQL, MongoDB, and MySQL, among others. In other words, Debezium abstracts away the complexity of dealing with unstandardized transaction logs, and provides standardization for changes to be captured and handled downstream.</p>
<p>Debezium, to the best of our knowledge, is the only open-source CDC tool that can capture from a variety of databases. Debezium&#x27;s in-depth documentation and wide usage make it a reasonable tool for Willow to use.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="42---apache-kafka">4.2 - Apache Kafka<a href="#42---apache-kafka" class="hash-link" aria-label="Direct link to 4.2 - Apache Kafka" title="Direct link to 4.2 - Apache Kafka">​</a></h3>
<p>Apache Kafka is an open-source, distributed event streaming platform.</p>
<p>While Kafka is a broad topic and has many moving parts, the core workflow is simple: <em>producers</em> send messages to Kafka brokers, which can then be processed by <em>consumers</em>. Within Willow, Kafka is a message broker that stores streams of Debezium events.</p>
<p>At its core, Apache Kafka consists of append-only logs, where messages are stored in sequential order. Kafka calls these logs <strong>topics</strong>. Topics are where <strong>events</strong> - records of a state change - are stored.</p>
<figure><img src="/img/case-study/4.2-kafka_log.svg" alt="Image demonstrating that a Kafka topic is essentially a log. The topic contains four events. The first event in the log is at position 0. The next event to be placed in the topic will be appended to the end at position 4."></figure>
<p>A single <strong>broker</strong>, or individual Kafka server, is responsible for storing and managing one or more topics. A group of brokers, called a <strong>cluster</strong>, work together to handle incoming events. Brokers within a cluster can be <strong>distributed</strong> across a network, and clustered brokers can replicate each others’ topics to provide data backups.</p>
<p>Because Kafka can be distributed across different servers and enable data replication, it is highly scalable and fault tolerant. Furthermore, its core data structure, an append-only log, enables fast reads and writes.</p>
<p>Apache Kafka clusters can be handled by <strong>Apache Zookeeper</strong>, which manages metadata on Kafka’s components. Zookeeper functions as a centralized controller.</p>
<figure><img src="/img/case-study/4.2-debezium_to_kafka.png" alt="Image demonstrating the high level relationship between Debezium and Kafka. Data flows from Debezium to Kafka. Kafka stores the data in topics. Data in topics is then read by consumers."><figcaption>High level relationship between Debezium and Kafka</figcaption></figure>
<p>We chose Apache Kafka for a few reasons. The first is that Debezium is natively built on top of it; there is wide support and documentation for streaming Debezium events to Kafka, and how such events can be processed by downstream consumers. The second, related reason is a tool called Kafka Connect. Kafka Connect is a framework that allows one to set up, update, and tear down source and sink connectors that use Kafka as a message broker.</p>
<p>Debezium has three deployment methods: Debezium Engine, Debezium Server, and deployment through Kafka Connect Debezium Server and Debezium Engine largely lack the ease of use provided by Kafka Connect’s REST API, and would require specifying a message broker. The third deployment method - deployment via Kafka Connect - streams changes directly to Apache Kafka. It provides an easy to use REST API for configuring and setting up connectors to Apache Kafka. Using Kafka Connect allows us to leverage Apache Kafka’s advantages, which includes persistence of records to disk in a way that is optimized for speed and efficiency.</p>
<figure><img src="/img/case-study/4.2-database_connect_kafka.svg" alt="Image demonstrating that Kafka Connect creates a Debezium connector between a database and Kafka."><figcaption>Debezium’s PostgreSQL source connector can be deployed via Kafka Connect, creating a connection between a database and Kafkaka</figcaption></figure>
<p>Apache Kafka’s robust features, along with the tools that surround it - most notably, Kafka Connect - make it a logical message broker to be used with Debezium.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="43---willow-adapter">4.3 - Willow Adapter<a href="#43---willow-adapter" class="hash-link" aria-label="Direct link to 4.3 - Willow Adapter" title="Direct link to 4.3 - Willow Adapter">​</a></h3>
<p>While Kafka enabled us to have a reliable streaming platform to publish database change events captured by Debezium, we still needed a way to consume those events and transform them into a suitable format for a cache. Initially, we researched a Redis Kafka Connect connector that consumes events from a Kafka topic and writes to a Redis cache. This connector converts events into Redis data types and generates a unique Redis key for each row. When assessing this connector, we were able to reflect <code>CREATE</code> and <code>UPDATE</code> changes to the cache, but <code>DELETE</code> changes were not reflected in the cache. Also, additional Kafka Connect metadata was inserted into the cache, which was undesirable since we only want database rows to be reflected in the cache.</p>
<p>The Redis Kafka Connector connector was not optimal for Willow&#x27;s needs, so we chose to implement our own custom Redis connector - a Willow Adapter for Kafka built with NodeJS. The Willow Adapter provides similar functionality to the Redis Kafka Connect connector, but it handles database level deletes appropriately by removing the associated data from the cache. The Willow Adapter also only inserts database row information - no Kafka Connect metadata is inserted. To consume messages from a Kafka topic, the KafkaJS npm package was used. To process those messages and update the cache, a custom class was created that leverages the Redis npm package.</p>
<p>In order to provide a user-friendly UI for building a CDC pipeline, the Willow Adapter both provides a React application and acts as the REST API for Willow’s UI, simplifying setup and teardown of each pipeline.</p>
<figure><img src="/img/case-study/4.3-nodejs_v2.svg" alt="Image demonstrating that data flows from Kafka into the Willow Adapter then into the sink cache."></figure>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="44---postgresql">4.4 - PostgreSQL<a href="#44---postgresql" class="hash-link" aria-label="Direct link to 4.4 - PostgreSQL" title="Direct link to 4.4 - PostgreSQL">​</a></h3>
<p>The final component of Willow&#x27;s architecture is a PostgreSQL database. PostgreSQL is an open-source, relational database management system (RDBMS) that stores structured data in tables. Data within PostgreSQL can be retrieved by using Structured Query Language (SQL) queries.</p>
<p>An RDBMS works well when data follows a well-defined format and associations exist between different data entities. Willow uses PostgreSQL for its RDBMS, which is appropriate since associations exist among Willow&#x27;s various entities; notably, each pipeline is associated with a source and sink. Configuration details for sources and sinks also follow a well-defined format, aligning with the type of structured data PostgreSQL excels at persisting. By storing pipeline information within a PostgreSQL database, Willow can redisplay existing pipeline information in its UI.</p>
<figure><img src="/img/case-study/4.4-postgres.svg" alt="Image demonstrating that a PostgreSQL database persists connection data for Willow."></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5---architecture">5 - Architecture<a href="#5---architecture" class="hash-link" aria-label="Direct link to 5 - Architecture" title="Direct link to 5 - Architecture">​</a></h2>
<p>As shown, Willow’s pipeline is built upon various open-source tools. The components can be summarized as follows:</p>
<figure><img src="/img/case-study/5-table.svg" alt="Table summarizing Willow&#x27;s individual architectural components."></figure>
<figure><img src="/img/case-study/5-architecture.svg" alt="Image showing Willow&#x27;s architecture. All components mentioned in section 4 are included."></figure>
<p>To minimize potential configuration issues with installing Willow, we use Docker. Containerizing Willow also makes sense from a long-term perspective; Docker supports various orchestration tools like Kubernetes or Docker Swarm that allows users to manage and scale containers. In other words, Docker is not only portable and consistent across various environments, but it is also horizontally scalable. Its ease of use and portability make it an important piece of Willow.</p>
<p>The final architecture is as follows:</p>
<figure><img src="/img/case-study/5-architecture_with_docker.svg" alt="Image showing Willow&#x27;s architecture including Docker."></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6---challenges">6 - Challenges<a href="#6---challenges" class="hash-link" aria-label="Direct link to 6 - Challenges" title="Direct link to 6 - Challenges">​</a></h2>
<p>Willow&#x27;s development encountered two main technical challenges: Debezium configuration and event transformation.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="61---debezium-configuration">6.1 - Debezium Configuration<a href="#61---debezium-configuration" class="hash-link" aria-label="Direct link to 6.1 - Debezium Configuration" title="Direct link to 6.1 - Debezium Configuration">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="611---multiple-pipelines-sharing-a-replication-slot">6.1.1 - Multiple Pipelines Sharing a Replication Slot<a href="#611---multiple-pipelines-sharing-a-replication-slot" class="hash-link" aria-label="Direct link to 6.1.1 - Multiple Pipelines Sharing a Replication Slot" title="Direct link to 6.1.1 - Multiple Pipelines Sharing a Replication Slot">​</a></h4>
<p>One of the challenges was centered around multiple pipelines sharing a replication slot. A replication slot is a PostgreSQL feature that keeps track of the last-read entry in the write-ahead log - PostgreSQL’s transaction log - for a specific consumer. For Willow, a consumer is equivalent to a pipeline.</p>
<p>In an early version, Willow reused a single replication slot for every pipeline connected to a PostgreSQL server, but only a single pipeline received changes and the remaining pipelines received none. This occurred since all pipelines for a single PostgreSQL server shared a replication slot and only one pipeline can consume from a replication slot at a time. However, pipelines connected to the same PostgreSQL server should be considered independent entities that consume all relevant changes.</p>
<figure><img src="/img/case-study/6.1.1-Multiple Pipelines Sharing Replication Slot A.gif" alt="Animation showing two pipelines sharing a replication slot. Data flows from the database&#x27;s write-ahead log and into the replication slot. Data flows from the replication slot to only one of the connected pipelines. The other pipeline does not receive any data."></figure>
<p>In order to ensure multiple pipelines using the same PostgreSQL server receive every change, Willow creates a unique replication slot for each pipeline. By doing so, Willow enables each pipeline to concurrently and separately read from the write-ahead log.</p>
<figure><img src="/img/case-study/6.1.1-Multiple Pipelines Separate Replication Slots.gif" alt="Animation showing two pipelines with separate replication slots. Data flows from the database&#x27;s write-ahead log and into the replication slots. Data flows from the replication slots to their associated pipelines."></figure>
<p>A tradeoff to this approach is that a new replication slot is created in the PostgreSQL server each time a pipeline is generated. This can result in multiple replication slots, creating opportunities for inactive slots when associated pipelines are torn down. Inactive replication slots force PostgreSQL to indefinitely retain all write-ahead log files unread by the associated pipeline, filling up disk space. Database administrators must carefully monitor and purge any inactive replication slots to avoid unnecessary write-ahead log retention.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="612---working-with-minimum-privileged-user">6.1.2 - Working with Minimum Privileged User<a href="#612---working-with-minimum-privileged-user" class="hash-link" aria-label="Direct link to 6.1.2 - Working with Minimum Privileged User" title="Direct link to 6.1.2 - Working with Minimum Privileged User">​</a></h4>
<p>The second challenge was ensuring source connectors are successfully created when Willow is provided a database user that does not have <code>SUPERUSER</code> privileges. A PostgreSQL <code>SUPERUSER</code> bypasses all permission checks and accesses everything in the server. A minimum privileged user, on the other hand, only has sufficient permissions for Willow to create a source connector and replicate specific tables within the PostgreSQL server. These privileges are <code>REPLICATION</code>, <code>LOGIN</code>, database level <code>CREATE</code>, and table level <code>SELECT</code> privileges.</p>
<p>Debezium successfully creates a source connector to a PostgreSQL server when the provided PostgreSQL user either is a <code>SUPERUSER</code> or a minimum privileged user. Willow follows the principle of least privilege - the idea of providing users and programs only the minimum level of access needed to perform their responsibilities - by allowing end users to provide a minimum privileged PostgreSQL user.</p>
<p>Debezium uses the minimum privileged user to create a publication. A <strong>publication</strong> is a database-scoped sequential list of <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>, and <code>TRUNCATE</code> changes for selected tables and are how pipelines receive data for what changes occur in those tables.</p>
<p>Initially, Willow worked well when provided a <code>SUPERUSER</code> but failed when given a minimum privileged user. The core issue came down to how Debezium creates publications in the source database and how Willow was configuring Debezium with a minimum privileged user.</p>
<p>Initially, Willow used Debezium&#x27;s <code>table.exclude.list</code> setting to specify which tables should not be replicated. By process of elimination, Debezium would then attempt to replicate all of the non-excluded tables in the database. However, if private tables exist in the database that the minimum privileged user does not have access to, these tables would not be visible when Willow queried the database to determine what tables exist, and those non-visible tables would not be listed in <code>table.exclude.list</code>. As a result, Debezium’s publication creation would fail since the publication contained tables inaccessible to the minimum privileged user.</p>
<figure><img src="/img/case-study/6.1.2-Minimum Privileged User_1.gif" alt="Animation showing Debezium attempting to create a publication with a minimum privileged user. Since the table.exclude.list setting is used, Debezium&#x27;s publication creation attempt fails."></figure>
<p>This issue was resolved by using Debezium&#x27;s <code>table.include.list</code> instead of its exclude counterpart. This strategy of white-listing instead of black-listing tells Debezium exactly which tables to include in the publication, preventing Debezium from including tables inaccessible to the minimum privileged user.</p>
<figure><img src="/img/case-study/6.1.2-Minimum Privileged User_2.gif" alt="Animation showing Debezium attempting to create a publication with a minimum privileged user. Since the table.include.list setting is used, Debezium&#x27;s publication creation attempt succeeds."></figure>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="62---event-transformation">6.2 - Event Transformation<a href="#62---event-transformation" class="hash-link" aria-label="Direct link to 6.2 - Event Transformation" title="Direct link to 6.2 - Event Transformation">​</a></h3>
<p>Events generated by Debezium take the shape of a key-value pair, representing an individual table’s row change. The key contains information about the row&#x27;s primary key value, and the value contains information about the type of change and the row&#x27;s updated values.</p>
<p>Willow uses a combination of the event’s key and value to determine the Redis key, which follows the format <code>database.table.primarykey</code>.</p>
<figure><img src="/img/case-study/6.2-transformation_process.svg" alt="Image demonstrating the transformation process to convert database write-ahead log entries into Redis key-value pairs."><figcaption>High-level proces of write-ahead log entries transforming into Redis key-value pairs.</figcaption></figure>
<p>When transforming events into Redis key-value pairs, Willow handles a few edge cases: tombstone events, tables without primary keys, and tables with composite primary keys.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="621---tombstone-events">6.2.1 - Tombstone Events<a href="#621---tombstone-events" class="hash-link" aria-label="Direct link to 6.2.1 - Tombstone Events" title="Direct link to 6.2.1 - Tombstone Events">​</a></h4>
<p>When a row is deleted, Debezium generates two events. The first is a <code>DELETE</code> event containing information about the deleted row. The second, called a tombstone event, has a <code>key</code> property containing the deleted row’s primary key and a <code>value</code> property with a <code>null</code> value.</p>
<p>Tombstone events are used by Apache Kafka to remove all previous records related to that row in a process called log compaction. Log compaction helps Kafka reduce the size of each topic while still retaining enough information to replicate the table&#x27;s current state.</p>
<p>Since tombstone events are used for Kafka’s log compaction, the Willow Adapter ignores them when transforming Debezium events to Redis key-value pairs.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="622---no-primary-key">6.2.2 - No Primary Key<a href="#622---no-primary-key" class="hash-link" aria-label="Direct link to 6.2.2 - No Primary Key" title="Direct link to 6.2.2 - No Primary Key">​</a></h4>
<p>A table&#x27;s primary key is a column that contains a unique, not <code>null</code> value and uniquely identifies a single row. For tables with no primary keys, events have <code>null</code> keys.</p>
<p>In this situation, Willow is opinionated and prevents tables without primary keys from being replicated. Willow’s UI does not show tables without a primary key as an option when selecting tables to replicate. Without a primary key, it is difficult to determine which Redis key-value pair should be created, updated or deleted.</p>
<p>The tradeoff of this decision is that Willow cannot be used for tables without a primary key. While these types of tables are sometimes used for containing message logs, their usage is infrequent when following good database design, and their contents are not typically desirable to replicate in a cache.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="623---composite-primary-key">6.2.3 - Composite Primary Key<a href="#623---composite-primary-key" class="hash-link" aria-label="Direct link to 6.2.3 - Composite Primary Key" title="Direct link to 6.2.3 - Composite Primary Key">​</a></h4>
<p>Instead of using a single column as the primary key, a table can use the combination of two or more columns to uniquely identify each row. This combination of columns is called a composite primary key.</p>
<p>In the visual below, the combination of the <code>order_id</code> and <code>payment_id</code> columns is the composite primary key for the Payments table.</p>
<figure><img src="/img/case-study/6.2.3-composite_pkey.svg" alt="Database table that has a composite primary key. The two columns, order_id and payment_id, that are used in the composite primary key are boxed in green."><figcaption>The Payments table has a comopsite primary key comprised of the <code>order_id</code> and <code>payment_id</code> columns (boxed in green).</figcaption></figure>
<p>For tables with a composite primary key, event keys contain information about all column values contributing to the row’s composite key.</p>
<p>Willow supports usage of composite primary keys by joining the individual values with a dot when creating the key for Redis&#x27; key-value pair, such as <code>database.table.keyvalue1.keyvalue2</code>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="7---conclusion-and-future-work">7 - Conclusion and Future Work<a href="#7---conclusion-and-future-work" class="hash-link" aria-label="Direct link to 7 - Conclusion and Future Work" title="Direct link to 7 - Conclusion and Future Work">​</a></h2>
<p>To conclude, Willow is an open-source, self-hosted event-driven framework with a specific use case. It utilizes log-based change data capture to build event streaming pipelines, capturing row-level changes from databases and updating caches in near real-time. It solves a specific form of the cache consistency problem and bypasses various existing solutions, such as TTL and polling. With its simple and intuitive UI, as well as the ability to select which tables and columns to capture, Willow fills a niche in the cache invalidation solution space.</p>
<p>While we are happy with Willow, there is room for improvement. The following are areas where we would like to expand upon in the future.</p>
<p><strong>Decoupling Source and Sink Connectors</strong></p>
<p>In our current architecture, pipelines have a one-to-one relationship with a source database and a sink cache. That is, when a user wants to create a pipeline, they must enter information for some source database and sink cache, even if that information has already been entered for another Willow pipeline. In the enterprise solutions we’ve seen, source connector information can be registered without only being  used for a single pipeline; one can specify a source connector that can then be used for multiple pipelines. The same holds for sink connectors.</p>
<p>Decoupling source and sink connectors would also enable Willow to stream changes to different caches without needing to establish new pipelines. Currently for each Willow pipeline, there is one source connector and one sink connector. In certain situations, users may want to stream changes from one source database to multiple Redis caches. To do so, for each additional cache, a user would need to create a new pipeline. However, this is not an ideal design, as each additional pipeline creates redundant Kafka topics, taking up additional disk space. By decoupling source and sink connectors, Willow users could simply tie multiple Redis sinks to the same source database in one pipeline. This would grant users more flexibility and be a more efficient use of resources.</p>
<p><strong>Deployment and Management</strong></p>
<p>Another feature we would like to enable is automatic deployment and management across distributed servers. Currently, Willow operates as a multi-container application on one server. While users can deploy Willow on multiple servers themselves, it would be valuable to have Willow provide that capability by default. This option would make Willow more easily available and fault-tolerant in case a container breaks. We would look into container orchestration tools like Kubernetes or Docker Swarm to implement this feature.</p>
<p><strong>Observability</strong></p>
<p>Currently, there are no observability metrics for monitoring the health and status of Willow’s pipelines. Apache Kafka, Apache Zookeeper, and Kafka Connect all enable Java Management Extensions (JMX), which are technologies that monitor and manage Java applications. Setting up JMX would provide metrics on various groups within the Willow architecture, including Kafka brokers, Zookeeper controllers, and Kafka consumers. Furthermore, in addition to JMX metrics, Debezium connectors provide ways to set up additional monitoring. These primarily provide snapshot and streaming metrics. Adding observability features would allow Willow’s end-users to be better informed and more easily identify issues.</p>
<p><strong>Other ideas</strong></p>
<p>Other features we would like to add include:</p>
<ul>
<li>The ability to connect more types of sources and sinks. Currently, Willow only captures changes from PostgreSQL databases into Redis caches.</li>
<li>The option to encrypt certain steps in the pipeline. For example, while we currently set up Debezium to prefer to use an encrypted connection to the source database, if no certifications are provided, Debezium can default to an unencrypted connection.</li>
<li>Adding more Redis types. Currently, rows are stored as Redis JSON types within the cache. While we believe this offers the most flexibility, allowing users to store as other types (Redis Hashes, Strings) can be beneficial.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="8---references">8 - References<a href="#8---references" class="hash-link" aria-label="Direct link to 8 - References" title="Direct link to 8 - References">​</a></h3>
<ol>
<li><a href="https://www.gigaspaces.com/blog/amazon-found-every-100ms-of-latency-cost-them-1-in-sales" target="_blank" rel="noopener noreferrer">Amazon Found Every 100ms of Latency Cost them 1% in Sales</a></li>
<li><a href="https://www.thinkwithgoogle.com/intl/en-emea/marketing-strategies/app-and-mobile/mobile-page-speed-data/" target="_blank" rel="noopener noreferrer">Google Study on Website Performance</a></li>
<li><a href="https://redis.com/blog/how-slow-queries-hurt-your-business/" target="_blank" rel="noopener noreferrer">How Slow Queries Hurt Your Business</a></li>
<li><a href="https://medium.com/@mmoshikoo/cache-strategies-996e91c80303" target="_blank" rel="noopener noreferrer">Cache Strategies</a></li>
<li><a href="https://www.geeksforgeeks.org/cache-invalidation-and-the-methods-to-invalidate-cache/" target="_blank" rel="noopener noreferrer">Cache Invalidation</a></li>
<li><a href="https://www.qlik.com/us/change-data-capture/cdc-change-data-capture" target="_blank" rel="noopener noreferrer">What is Change Data Capture</a></li>
<li><a href="https://www.pubnub.com/blog/how-fast-is-realtime-human-perception-and-technology/" target="_blank" rel="noopener noreferrer">How Fast is Real-Time?</a></li>
<li><a href="https://www.confluent.io/learn/change-data-capture/" target="_blank" rel="noopener noreferrer">Change Data Capture Methods</a></li>
<li><a href="https://debezium.io/" target="_blank" rel="noopener noreferrer">Debezium</a></li>
<li><a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Apache Kafka</a></li>
<li><a href="https://www.postgresql.org/" target="_blank" rel="noopener noreferrer">PostgreSQL</a></li>
<li><a href="https://www.postgresql.org/docs/10/logicaldecoding-explanation.html#LOGICALDECODING-REPLICATION-SLOTS" target="_blank" rel="noopener noreferrer">PostgreSQL Replication Slots</a></li>
<li><a href="https://debezium.io/documentation/reference/2.5/connectors/postgresql.html#postgresql-permissions" target="_blank" rel="noopener noreferrer">Minimum Privileged User Permissions</a></li>
<li><a href="https://debezium.io/documentation/reference/2.5/connectors/postgresql.html#postgresql-delete-events" target="_blank" rel="noopener noreferrer">Tombstone Events</a></li>
<li><a href="https://www.postgresql.org/docs/current/ddl-constraints.html#DDL-CONSTRAINTS-PRIMARY-KEYS" target="_blank" rel="noopener noreferrer">Primary Keys</a></li>
<li><a href="https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-replica-identity" target="_blank" rel="noopener noreferrer">Usage of Tables with No Primary Keys</a></li>
</ol></article></div><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#0---introduction" class="table-of-contents__link toc-highlight">0 - Introduction</a></li><li><a href="#1---background" class="table-of-contents__link toc-highlight">1 - Background</a><ul><li><a href="#11---general-problem-domain" class="table-of-contents__link toc-highlight">1.1 - General Problem Domain</a></li><li><a href="#12---caching" class="table-of-contents__link toc-highlight">1.2 - Caching</a></li><li><a href="#13---change-data-capture" class="table-of-contents__link toc-highlight">1.3 - Change Data Capture</a></li></ul></li><li><a href="#2---existing-solutions" class="table-of-contents__link toc-highlight">2 - Existing Solutions</a><ul><li><a href="#21---enterprise-solutions" class="table-of-contents__link toc-highlight">2.1 - Enterprise Solutions</a></li><li><a href="#22---diy-solutions" class="table-of-contents__link toc-highlight">2.2 - DIY Solutions</a></li></ul></li><li><a href="#3---introducing-willow" class="table-of-contents__link toc-highlight">3 - Introducing Willow</a><ul><li><a href="#31-demonstration" class="table-of-contents__link toc-highlight">3.1 Demonstration</a></li><li><a href="#32---using-willow" class="table-of-contents__link toc-highlight">3.2 - Using Willow</a></li></ul></li><li><a href="#4---implementation" class="table-of-contents__link toc-highlight">4 - Implementation</a><ul><li><a href="#41---debezium" class="table-of-contents__link toc-highlight">4.1 - Debezium</a></li><li><a href="#42---apache-kafka" class="table-of-contents__link toc-highlight">4.2 - Apache Kafka</a></li><li><a href="#43---willow-adapter" class="table-of-contents__link toc-highlight">4.3 - Willow Adapter</a></li><li><a href="#44---postgresql" class="table-of-contents__link toc-highlight">4.4 - PostgreSQL</a></li></ul></li><li><a href="#5---architecture" class="table-of-contents__link toc-highlight">5 - Architecture</a></li><li><a href="#6---challenges" class="table-of-contents__link toc-highlight">6 - Challenges</a><ul><li><a href="#61---debezium-configuration" class="table-of-contents__link toc-highlight">6.1 - Debezium Configuration</a></li><li><a href="#62---event-transformation" class="table-of-contents__link toc-highlight">6.2 - Event Transformation</a></li></ul></li><li><a href="#7---conclusion-and-future-work" class="table-of-contents__link toc-highlight">7 - Conclusion and Future Work</a><ul><li><a href="#8---references" class="table-of-contents__link toc-highlight">8 - References</a></li></ul></li></ul></div></div></div></main></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Navigation</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/case-study">Case Study</a></li><li class="footer__item"><a class="footer__link-item" href="/#team">Team</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/Prerequisites">Docs</a></li><li class="footer__item"><a href="https://github.com/willow-cdc" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github</a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><img src="/img/logo.svg" alt="Willow Logo" class="footer__logo themedComponent_mlkZ themedComponent--light_NVdE" width="80"><img src="/img/logo.svg" alt="Willow Logo" class="footer__logo themedComponent_mlkZ themedComponent--dark_xIcU" width="80"></div><div class="footer__copyright">Copyright © 2024 Willow</div></div></div></footer></div>
</body>
</html>